---
title: "Допущения модели линейной регрессии и линеаризация зависимостей"
author: "Заходякин Г.В., postlogist@gmail.com" (slightly modified by Rozhkov M.I. max-over@yandex.ru)
date: "14.02.2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
options(width = 100) # ширина текстового вывода
options(digits = 3) # число знаков после запятой в выводе 
```

```{r Загрузка пакетов, message=FALSE}
library(readr) # считывание данных из текстовых файлов
library(tidyverse) # манипулирование данными 
library(ggplot2) # визуализация 
library(ggfortify) # визуализация диагностических графиков
library(modelr) # вспомогательные функции для работы с моделями
library(broom) # преобразование результатов моделирования в табличный вид
library(GGally) # построение матрицы диаграмм рассеяния 
library(car) # функции для степенных преобразований
```


# Модель линейной регрессии

https://rawgit.com/postlogist/research-seminar/master/linear-regression/regression_assumptions.html

Модель линейной регрессии позволяет предсказать **зависимую** переменную (dependent/output variable) на основе одной **объясняющей** переменной (explanatory/predictor variable). При этом предполагается, что связь между переменными линейна и для генеральной совокупности справедливо уравнение:

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

Параметры $\beta_0$ и $\beta_1$ определяют соответственно **свободный член** (intercept) и **угловой коэффициент** (slope) прямой линии. Свободный член $\beta_0$ - это ожидаемое значение $y$ при $x=0$. Угловой коэффициент $\beta_1$ отражает ожидаемое изменение $y$ при единичном увеличении $x$.

**Остаток** модели (error term) $\varepsilon$ включает отклонения отдельных наблюдений от предсказанных моделью ожидаемых значений, т.е. ошибку модели. Таким образом, каждое наблюдение  $y_i$ можно рассматривать как сумму двух компонентов: закономерного (объясненного моделью):  

$$\beta_0 + \beta_1 x_i,$$ 


и случайной ошибки: $$\varepsilon_i.$$

Остаток модели включает действие всех факторов, влияющих на $y$, но не включенных в модель - т.е. всех факторов, кроме $x$.

# Допущения модели линейной регрессии

Остаток $\varepsilon$ - это важный компонент модели линейной регрессии. Именно свойства остатков используются для статистического вывода (оценки значимости модели) и интервального прогноза. Если аналитик хочет воспользоваться этими методами, то он должен убедиться в том, что исходные предположения модели не нарушены.


Перечислим эти предположения:

1. **Среднее значение остатка - нулевое** (нет систематической ошибки) 

$$E(\varepsilon)=0$$

2. **Остатки модели некоррелированы между собой**:

$$\rho(\varepsilon_i, \varepsilon_k) = 0, \forall i \ne k$$

Если данное предположение нарушается, то модель будет неэффективной, т.е. можно построить более точную модель на тех же данных. Причина в том, что корреляция остатков между собой - это полезная информация, которая может быть учтена в модели для снижения ошибки. 


3. **Остатки модели некоррелированы с объясняющей переменной**:

$$\rho(\varepsilon, x_j) = 0, \forall x_j$$

Это предположение распространяется и на другие переменные в наборе данных. Если есть зависимость остатков от другой переменной, то можно улучшить точность модели, включив в нее эту переменную. 


4. **Дисперсия (и стандартное отклонение) остатков постоянна**, т.е. не зависит от $x$:

$$\sigma_\varepsilon = const, \forall x$$

Данное допущение называется **гомоскедастичностью** (homoscedasticity).

5. **Распределение остатков модели - нормальное**: $$\varepsilon \sim N(0, \sigma).$$


Эти предположения лежат в основе статистического вывода по модели и получения интервального прогноза.


Anscombe data

```{r Преобразование структуры данных anscombe} 

anscombe_tall <- anscombe %>%
 pivot_longer(everything(),
   names_to = c(".value", "case"),
   names_pattern = "(.)(.)"
 )
head(anscombe_tall, 30)
anscombe_tall2 <- anscombe_tall
```

```{r Визуализация данных anscombe} 
ggplot(data = anscombe_tall, mapping = aes(x, y)) + 
  geom_point() + 
  facet_wrap(~ case, ncol = 2, scales = 'free_x') + 
  geom_smooth(method = 'lm', se = F, color = 'red') +
  labs(title = "Примеры зависимостей с одинаковым r = 0.82")
```

```{r}
install.packages("gvlma")
library(gvlma)
```
gvlma package helps to check regression model assumptions.

Examples:
https://stackoverflow.com/questions/43252474/using-and-interpreting-output-from-gvlma
https://cran.r-project.org/web/packages/gvlma/gvlma.pdf
https://rdrr.io/cran/gvlma/man/00gvlma.package.html
http://analyticswithr.com/ols.html
https://joshualoong.com/2019/09/20/Appreciating-R-The-Ease-of-Testing-Linear-Model-Assumptions/

1.Global Stat- Are the relationships between your X predictors and Y roughly linear?. Rejection of the null (p < .05) indicates a non-linear relationship between one or more of your X’s and Y

2.Skewness - Is your distribution skewed positively or negatively, necessitating a transformation to meet the assumption of normality? Rejection of the null (p < .05) indicates that you should likely transform your data.

3.Kurtosis- Is your distribution kurtotic (highly peaked or very shallowly peaked), necessitating a transformation to meet the assumption of normality? Rejection of the null (p < .05) indicates that you should likely transform your data.

4.Link Function- Is your dependent variable truly continuous, or categorical? Rejection of the null (p < .05) indicates that you should use an alternative form of the generalized linear model (e.g. logistic or binomial regression).

5.Heteroscedasticity- Is the variance of your model residuals constant across the range of X (assumption of homoscedastiity)? Rejection of the null (p < .05) indicates that your residuals are heteroscedastic, and thus non-constant across the range of X. Your model is better/worse at predicting for certain ranges of your X scales.



Anscombe dataset analysis:

```{r}
lmA1=lm(anscombe[, 5] ~ anscombe[ , 1])
summary(lmA1)
gv_lmA1 <- gvlma(lmA1)
summary(gv_lmA1)
```
```{r}
lmA2=lm(anscombe[, 6] ~ anscombe[ , 2])
summary(lmA2)
gv_lmA2 <- gvlma(lmA2)
summary(gv_lmA2)
```
```{r}
lmA3=lm(anscombe[, 7] ~ anscombe[ , 3])
summary(lmA3)
gv_lmA3 <- gvlma(lmA3)
summary(gv_lmA3)
```

```{r}
lmA4=lm(anscombe[, 8] ~ anscombe[ , 4])
summary(lmA4)
gv_lmA4 <- gvlma(lmA4)
summary(gv_lmA4)
```


Testing old costs model

```{r Загрузка пакетов, message = F} 
library(readxl)
```


```{r Загрузка данных, message = F} 
volume_costs <- read_xlsx('test_dataset.xlsx', sheet='vol_costs')
```

```{r}
m_volume_costs <- lm(Costs ~ Volume, data = volume_costs) 
coef(m_volume_costs) 
```
p-test in gvlma package is different from "usual" p-test. Here higher p value means more model "fit":

```{r}

gv_m_volume_costsl <- gvlma(m_volume_costs)
summary(gv_m_volume_costsl )
plot(gv_m_volume_costsl )
```

Generating test data:

```{r Визуализация условных распределений, echo=F}
beta1 <- 0.5
beta0 <- 10
sd_epsilon <- .5
y <- seq(2.5,12.5,by=0.5)
# y = beta0 + beta1 * x => x = (y - beta0) / beta1
x <- (y - beta0) / beta1
ploterr <- function(y, mean, beta0, beta1, sd) {
  dx <- (mean - beta0) / beta1
  dnorm(y, mean, sd) + dx
}
tibble(x,y)


ggplot(data = tibble(x, y), mapping = aes(y, x)) +
         geom_line() + 
  stat_function(fun = ploterr, 
                args = list(5, beta0, beta1, sd_epsilon), col = 'skyblue') +
  stat_function(fun = ploterr, 
                args = list(7.5, beta0, beta1, sd_epsilon), col = 'skyblue') +
  stat_function(fun = ploterr, 
                args = list(10, beta0, beta1, sd_epsilon), col='skyblue') +
  coord_flip() +
  geom_text(aes(x = 8.0, y = 2.5, 
                label = "y == beta[0] + beta[1] * x + epsilon"), parse = T,
            size = 6) +
  geom_text(aes(x = 7.0, y = 2.5, 
                label = "epsilon %~% N(0, sigma)"), parse = T,
            size = 6) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank())
```





# Проверка допущений модели линейной регрессии (диагностика остатков)

Для диагностики остатков используются стандартные графики, которые можно получить, применив к модели функцию `autoplot()` (должны быть загружены пакеты `ggplot2` и `ggfortify`) или встроенную функцию `plot` (в этом случае будет использоваться базовая графика R, вместо ggplot2). 




## Пример 1: хорошая модель, никакие допущения не нарушены

Генерация данных

https://renenyffenegger.ch/notes/development/languages/R/functions/runif

```{r Генерация выборки для хорошей модели}
# Параметры модели:
beta0 <- 5
beta1 <- 1
sd_eps <- 2
# Генерация данных:
set.seed(123)
N <- 200
x <- runif(N, 0, 10)
eps <- rnorm(N, 0, sd_eps)
y <- beta0 + beta1 * x + eps
```




Построение модели


```{r Построение модели на хороших данных}
m <- lm(y ~ x)
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = lm, se = F, col = 'blue') +
  labs(title = 'Хорошая модель: никакие допущения не нарушаются')
summary(m)
```

Диагностические графики для хорошей модели

Про выбор и настройку внешнего вида диагностических графиков можно прочитать [здесь](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_lm.html) и в справке: `?autoplot.lm`

https://cran.r-project.org/web/packages/ggfortify/ggfortify.pdf

```{r Диагностические графики для хорошей модели}
autoplot(m)
```

Первый график показывает зависимость остатков модели $e$ от предсказанных значений  $\hat{y}$.

Если допущения линейной регрессии верны, то не должно быть зависимости остатков от предскзанных значений. Искривленная форма облака точек на этом графике означает, что зависимость между $x$ и $y$ нелинейная и необходимо либо включать в модель нелинейные члены (степени $x$), либо использовать линеаризующее преобразование данных.  На этом графике будет видна и гетероскедастичность, т.е. непостоянство дисперсии остатков. Если гетероскедастичность присутствует, то доверительные интервалы для прогнозов окажутся чрезмерно оптимистичными (их построение основано на допущении о том, что дисперсия остатков постоянна), также некорректными будут и результаты проверки гипотез о значимости модели и коэффициентов.

Наблюдения с наибольшими значениями остатков помечаются номерами строк (можно выбрать другую переменную для текста метки, см. `?autoplot.lm`).


Второй диагностический график позволяет проверить предположение о нормальности остатков:

```{r}
autoplot(m, which = 2)
```

Это квантильный график для остатков. При справедливости допущения остатки на графике должны быть близкими к наклонной прямой линии.


Третий диагностический график позволяет обнаружить гетероскедастичность.

```{r}
autoplot(m, which = 3)
```

Если дисперсия остатков не постоянна, то облако точек будет искривленным или иметь воронкообразную форму. Здесь видно, что форма облака практически не изменяется в зависимости от $\hat{y}$, т.е. предположение о гомоскедастичности не нарушено.

Четвертый график помогает выявить выбросы и влиятельные наблюдения.

```{r}
autoplot(m, which = 5)
```


**Выброс** (outlier) - это наблюдения с большим остатком, т.е. наблюдение, для которого модель работает плохо.

**Влиятельное наблюдение** (influential observation) - наблюдение, которое сильно влияет на коэффициенты модели и полученный прогноз.


**Леверидж** (leverage) - это количественный показатель степени влияния наблюдения на модель - см. https://en.wikipedia.org/wiki/Leverage_(statistics). 

Link for whose preferring watching videos during seminar:

https://www.youtube.com/watch?v=xc_X9GFVuVU

Для расчета левериджа используется только значения объясняющих переменных $\mathbf X$. Эта величина показывает, насколько необычно значение $x$, т.е. насколько оно удалено от других с учетом только объясняющих переменных.

Другим способом оценить степень влияния наблюдения является **расстояние Кука** (Cook's Distance) - см. https://en.wikipedia.org/wiki/Cook's_distance. Эта величина характеризует степень изменения коэффициентов модели, если данное наблюдение исключить при оценке модели. Влиятельными считаются наблюдения с $D > 1$, по другой оценке - с $D > 4/ n$, где $n$ - число наблюдений.

No good video about Cook's distance on youtube, sorry

Можно вывести диагностический график, на котором для каждого наблюдения показаны обе эти величины.

https://wiki.q-researchsoftware.com/wiki/Regression_-_Diagnostic_-_Plot_-_Cook%27s_Distance_vs_Leverage


```{r}
autoplot(m, which = 6)
```

Влиятельные наблюдения необходимо внимательно изучить, поскольку они могут быть следствием ошибок при сборе или вводе данных и исказить результаты моделирования.

Model test by gvlma:
```{r}
gvmodel <- gvlma(m)
summary(gvmodel)
plot(gvmodel)
```

Given a gvlma object, which contains in the component GlobalTest the test statistics and p-values
for the global and directional tests to assess linear models assumptions, deletion.gvlma computes
the leave-one-out global and directional statistics. The deletion statistics are reported as percent
relative change from the corresponding statistic value based on the full data set.

```{r}

gvmodelDel <- deletion.gvlma(gvmodel)
plot(gvmodelDel)
summary(gvmodelDel)

```


## Плохая модель: нелинейность

В этом примере мы сгенерируем случайные данные с использованием нелинейной зависимости:

```{r Данные с нелинейной зависимостью}
beta2 <- .5
# Добавим квадратичный член:
y_nonlin <- beta0 + beta1 * x + beta2 * x^2 + eps
```


Оценка модели:

```{r Оценка модели для нелинейной зависимости в данных}
m_nonlin <- lm(y_nonlin ~ x) 
ggplot(tibble(x, y_nonlin), aes(x, y_nonlin)) +
  geom_point() +
  geom_smooth(method = lm, se = F, col = 'blue') +
  labs(title = 'Плохая модель: нелинейность')
summary(m_nonlin)
```

```{r Диагностика модели с нелинейными данными}
autoplot(m_nonlin) 
```

На первом диагностическом графике хорошо видно искривление облака точек. На квантильном графике отклонение от нормального распределения в области отрицательных остатков более заметно. На графике scale-location видно, что есть характерное изменение дисперсии остатков.

## Плохая модель 2: гетероскедастичность

В этом примере мы сгенерируем данные, в которых дисперсия остатков растет с увеличением $x$

```{r Данные с гетероскедастичностью}
y_hetero <- beta0 + beta1 * x + eps * x / mean(x)
```

Оценка модели:

```{r Модель с гетероскедастичностью}
m_hetero <- lm(y_hetero ~ x) 
ggplot(tibble(x, y_hetero), aes(x, y_hetero)) +
  geom_point() +
  geom_smooth(method = lm, se = F, col='blue') +
  labs(title = 'Плохая модель: гетероскедастичность')
summary(m_hetero)
```

```{r Диагностический график для модели с гетероскедастичностью}
autoplot(m_hetero)
```

На первом и третьем графиках видно, что облако точек имеет воронкообразную форму.

## Автокорреляция остатков

Случай взаимной зависимости (автокорреляции) остатков будет рассмотрен в лекции про регрессию для временных рядов.




# Что делать, если предположения линейной модели нарушаются?

Если проблема связана с нелинейностью данных или гетероскедастичностью, то можно попробовать преобразовать данные, чтобы минимизировать проявления этих нарушений.

## Преобразования для устранения гетероскедастичности

Непостоянство дисперсии - это распространенное явление в реальных данных, хотя оно может присутствовать в гораздо менее выраженной форме. Например, при увеличении зависимой переменной - дохода людей - увеличивается и степень неравенства между ними (дисперсия). 

Для устранения гетероскедастичности можно применять к зависимой переменной нелинейные преобразования:
 - логарифм (logarithm transformation)
 - квадратный корень (square root transformation)
 - обратное значение (inverse transformation)

```{r Plotting transformations}
# Исходные данные
ggplot(tibble(x, y_hetero), aes(x, y_hetero)) +
  geom_point() +
  geom_smooth(method = lm, se = F, col='blue') +
  labs(title = 'Плохая модель: гетероскедастичность')
# Логарифмирование
ggplot(tibble(x, y = log10(y_hetero)),
       aes(x, y))  +
         geom_point() +
         geom_smooth(method=lm, se=F, col='blue') +
         labs(title='Логарифмическое преобразование для данных с гетероскедастичностью',
              y = 'log10(y)')
# Корень
ggplot(tibble(x, y = sqrt(y_hetero)),
       aes(x, y)) +
  geom_point() + 
  geom_smooth(method=lm, se=F, col='blue') +
  labs(title='Преобразование квадратного корня для данных с гетероскедастичностью',
              y = 'sqrt(y)')
# Обратное значение
ggplot(tibble(x, y = 1/y_hetero),
       aes(x, y)) +
  geom_point() +
  geom_smooth(method = lm, se = F, col='blue') +
  labs(title='Обратное преобразование для данных с гетероскедастичностью',
       y = '1/y')
```

Данные  с обратным преобразованием выглядят наиболее привлекательно, попробуем оценить модель:

```{r Модель на преобразованных данных}
m_inv <- lm(1/y_hetero ~ x)
coef(m_inv)
```

Полученная модель имеет вид:

$$ \frac{1}{\hat{y}} = 0.1654 - 0.0108 x $$

или:

$$ {\hat{y}} = (0.1654 - 0.0108 x)^{-1} $$

Диагностика остатков:

```{r Residuals for the inverse-transformed data}
autoplot(m_inv)
```

Ситуация улучшилась, но все равно видно, что предположения линейности и гомоскедастичности нарушены.

## Нелинейность

Если между переменными $x$ и $y$ есть нелинейная зависимость, то и модель должна быть нелинейной. Для построения такой модели можно преобразовать зависимую переменную или предиктор, либо добавить в модель члены более высоких порядков - например, квадратичные. В этом случае придется использовать модель множественной регрессии.

```{r Полиномиальная регрессия}
m_poly <- lm(y_nonlin ~ x + I(x^2))
```

Для добавления квадратичного члена $x^2$ необходимо заключить его в функцию `I()` (identity), т.к. символ `^` имеет специальное назначение в синтаксисе формул R.


```{r Коэффициенты полиномиальной модели}
coef(m_poly)
```

Формула для квадратичной модели:

$$ \hat{y} = 5.233 + 0.930 x + 0.504 x^2 $$



```{r Визуализация квадратичной модели}
ggplot(data = add_predictions(tibble(x, y_nonlin), m_poly), 
      mapping = aes(x)) +
      geom_point(aes(y = y_nonlin)) +
      geom_line(aes(y = pred), colour='blue') +
  labs(title = 'Квадратичная зависимость', y = 'y')
```

Диагностика остатков для квадратичной модели

```{r Диагностика остатков квадратичной модели}
autoplot(m_poly)
```

Теперь графики не демонстрируют нарушений предположений линейной модели.

## Степенные преобразования

Как и в случае временных рядов, к данным для построения регрессионных моделей можно применять степенные преобразования (power transformations), обобщением которых является преобразование Бокса-Кокса:

$$ y \to y ^\lambda $$


Наиболее часто используемые значения параметра  степени преобразования $\lambda$ приведены в таблице.

$\lambda$   | Преобразование
------------|---------------
$-2$        | $1/y^2$
$-1$        | $1/y$
$-0.5$      | $1/\sqrt{y}$
$0$         | $\log{y}$
$0.5$       | $\sqrt{y}$
$1$         | Нет преобразования
$2$         | $y^2$



Для подбора степени преобразования данных для регрессии можно использовать функцию `car::powerTransform()`. Эта функция подбирает параметр $\lambda$ таким образом, чтобы распределение остатков линейной модели в результате было наиболее близко к нормальному. 

```{r Степенные преобразования для модели с гетероскедастичностью}
# Подбор степени для модели с гетероскедастичностью
t1 <- powerTransform(y_hetero ~ x)
coef(t1)
```

Степень для данных с гетероскедастичностью оказалась близкой к $-0.25$, что эквивалентно: $$\frac{1}{\sqrt[4]{y}}.$$ 


```{r Model for transformed heteroscedastic data}
m_ht <- lm(y_hetero^-0.25 ~ x)
coef(m_ht)
```

Уравнение регрессии:
$$ \frac{1}{\sqrt[4]{\hat{y}}} = 0.624 - 0.0141 \cdot x$$

или:

$$ \hat{y} = (0.624 - 0.0141 \cdot x)^{-4}$$

Диагностика остатков:


```{r Преобразованные данные с гетероскедастичностью - остатки модели}
autoplot(m_ht)
```



Рассмотрим данные с нелинейной зависимостью.

```{r Подбор степени для данных с нелинейной зависимостью}
## Подбор степени для нелинейной модели
t2 <- powerTransform(y_nonlin ~ x)
coef(t2)
```

Для нелинейной модели степень преобразования близка к $0.5$, т.е. $\sqrt{y}$.

```{r Модель для преобразованных данных с нелинейной зависимостью}
m_nlt <- lm(sqrt(y_nonlin) ~ x)
coef(m_nlt)
```

Уравнение регрессии:

$$ \sqrt{\hat{y}} = 1.766 + 0.613 \cdot x$$

или:

$$ \hat{y} = (1.766 + 0.613 \cdot x)^2$$

Диагностика остатков для этой модели:

```{r}
autoplot(m_nlt)
```

По графикам видно, что проблемы устранены не в полной мере.


## Как делать прогноз при использовании преобразований

### Если в модель включены нелинейные члены

В случае, когда преобразования объясняющей переменной или члены более высоких порядков были заданы непосредственно в формуле внутри `lm()`, то все необходимые преобразования данных R выполнит автоматически.

Рассмотрим пример с полиномиальной регрессией

Формула и коэффициенты модели:
```{r}
formula(m_poly)
coef(m_poly)
```

Точечный прогноз будет получен автоматически, например, с помощью функции `add_predictions()`. Интервальный прогноз можно получить с помощью функции `predict()` и добавить в таблицу данных для визуализации.

```{r Интервальный прогноз по квадратичной модели}
# Создаем точки для получения нового прогноза
newdata_nl <- data.frame(x = seq_range(x, by = 0.1))
# Интервальный прогноз:
pred_nl <- predict(m_poly, newdata = newdata_nl, 
                   interval = 'prediction',
                   level = .95)
# Соединяем все в одну таблицу:
newdata_nl <- bind_cols(newdata_nl, as.data.frame(pred_nl))
# График
ggplot(data = newdata_nl,
       mapping = aes(x)) +
  geom_ribbon(aes(ymin=lwr, ymax=upr), 
              fill='lightskyblue', alpha=1/2) +
  geom_line(aes(y = fit), colour='blue') +
  geom_point(data= tibble (x=x, y=y_nonlin), aes(x, y)) +
  labs(title='Интервальный прогноз по квадратичной модели (95% интервал)',
       y = 'y')
```


### Если преобразование применялось к зависимой переменной

В случае, когда преобразование применялось к зависимой переменной, линейная модель предсказывает преобразованное значение выходной переменной. Его нужно привести к исходной шкале путем обратного преобразования.

Например, если при моделировании использовалось преобразование: 
$$w = 1/\sqrt[4]{y},$$ 

то необходимо выполнить обратное преобразование прогноза и границ доверительного интервала по формуле: 

$$ \hat{y} = (1/w)^4.$$



Формула и коэффициенты модели с преобразованным $y$:

```{r }
formula(m_ht)
coef(m_ht)
```


Вычисление прогноза:

```{r Интервальный прогноз по модели с преобразованной зависимой переменной}
# Создаем точки для получения нового прогноза
newdata_ht <- data.frame(x = seq_range(x, by = 0.1))
# Интервальный прогноз:
pred_ht_transformed <- predict(m_ht, newdata = newdata_ht, 
                   interval = 'prediction',
                   level = .95)
# Обратное преобразование:
pred_ht <- pred_ht_transformed^(-4)
# Соединяем все в одну таблицу:
newdata_ht <- bind_cols(newdata_ht, as.data.frame(pred_ht))
# График
ggplot(data = newdata_ht,
       mapping = aes(x)) +
  geom_ribbon(aes(ymin=lwr, ymax=upr), 
              fill='lightskyblue', alpha=1/2) +
  geom_line(aes(y = fit), colour='blue') +
  geom_point(data= tibble (x=x, y=y_hetero), aes(x, y)) +
  labs(title='Интервальный прогноз по нелинейной модели (95% интервал)',
       y = 'y')
```

Границы доверительного интервала расходятся, т.к. к ним применили нелинейное преобразование. Это означает, что уровень неопределенности возрастает при увеличении $y$. Этого следовало ожидать, т.к. гетероскедастичность - по определению рост дисперсии. 


# Приложения
## Интерпретация коэффициентов модели при логарифмическом преобразовании данных
Логарифмические преобразования широко применяются для линеаризации зависимостей в данных и уменьшения гетероскедастичности. При использовании логарифмических и полулогарифмических моделей коэффициенты имеют специальную интерпретацию, приведенную в таблице

Уравнение  | Интерпретация углового коэффициента $b_1$
-- | --- 
$y = b_0 + b_1 x + e$      | При увеличении $x$ на $1$, $y$ изменится на  $b_1$ 
$\log y = b_0 + b_1 x + e$ | При увеличении $x$ на $1$, $y$ изменится на  $100 b_1 \%$  
$y = b_0 + b_1 \log x + e$ | При увеличении $x$ на $1 \%$, $y$ изменится на  $b_1 / 100$ 
$\log y = b_0 + b_1 \log x + e$ | При увеличении $x$ на $1\%$,  $y$ изменится на $b_1\%$ 



Модель 4 называется логарифмической, модели 2 и 3 - полу-логарифмическими (semi-log). В английском языке для обозначения перечисленных типов моделей моделей используются также термины: level-level, log-level, level-log и log-log. 

**Примечание**: лоагрифмическая модель (log-log) позволяет оценить **эластичность**   $y$ к $x$, т.е. процентное изменение  $y$ при изменении на $1\%$ переменной $x$.

## Пример поиска влиятельных наблюдений

Добавим в исходные данные несколько выбросов:

```{r Finding influential observations}
data_original <- tibble(x, y)
data_outliers <- 
  tribble(
    ~x, ~y,
    5,  1,
    10, 1,
    15, 1
  )
data_combined <- bind_rows(data_original, data_outliers)
ggplot(data_combined, aes(x, y)) + 
  geom_smooth(method = 'lm', colour = 'blue', se = F) +
  geom_point() +
  geom_smooth(data = data_original, 
              method = 'lm', color = 'blue', se = F, linetype = 'dashed') +
  geom_point(data = data_outliers, 
             mapping = aes(x, y),
             color = 'red',
             size = 5,
             alpha = 0.5) +
  geom_text(data = data_outliers, 
             mapping = aes(x, y, 
                           label = nrow(data_original) + 1:nrow(data_outliers)),
             color = 'red',size = 5,
            vjust = -1) +
  labs(title = 'Выбросы в данных')
```


Выбросы на диагностических графиках

```{r Диагностика остатков модели с выбросами}
# Построение модели
m_o <- lm(y ~ x, data = data_combined)
# Диагностические графики
autoplot(m_o, which = c(1, 4:6))
```

Для всех добавленных точек-выбросов остатки большие. Однако наблюдение **#201** $(5, 1)$  имеет небольшой леверидж, т.к. оно находится вблизи среднего значения $x$ и вокруг нее много других точек. Поэтому влияние этого наблюдения на модель небольшое: положение прямой в этой области определяется по большому числу наблюдений. Это наблюдение не помечено как влиятельное.

Наблюдение**#202** $(10, 1)$ находится дальше от центра, а ошибка модели в этой точке еще выше. Наибольшее удаление от других наблюдений имеет точка **#203** $(15, 1)$. У нее нет соседних точек, поэтому положение прямой в этой области определяется только на основе этого наблюдения. По этой причине данное наблюдение имеет большой леверидж и большое расстояние Кука. Некоторые источники рекомендуют использовать в качестве порога значение расстояния Кука $D=1$, другие - более консервативную границу $4/n$, где $n$ - число наблюдений в выборке.



More linear model assumptions:
http://r-statistics.co/Assumptions-of-Linear-Regression.html